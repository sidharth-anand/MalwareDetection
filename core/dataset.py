import abc
import typing
import random

import numpy as np
import tensorflow as tf

from core.config import DatasetConfig
from core.utils import DatasetUtils

class Dataset(metaclass=abc.ABCMeta):
    DatasetType = typing.List[typing.Tuple[str, int]]

    def __init__(self, path: str, config: DatasetConfig, utils: DatasetUtils) -> None:
        self.path = path
        self.config = config
        self.utils = utils

        self.train_data: tf.data.Dataset = None
        self.validation_data: tf.data.Dataset = None
        self.test_data: tf.data.Dataset = None

        self.classes: typing.Dict[int, str] = {}

        self._build_dataset()

    @property
    def num_classes(self) -> int:
        return len(self.classes)

    @abc.abstractmethod
    def _get_dataset(self) -> DatasetType:
        pass

    @abc.abstractmethod
    def _get_classes(self) -> typing.List[str]:
        pass

    def _build_dataset(self) -> None:
        dataset = self._get_dataset()
        dataset_length = len(dataset)

        if 0 < self.config.max_size < dataset_length:
            dataset = random.sample(dataset, self.config.max_size)
            dataset_length = self.config.max_size

        random.shuffle(dataset)

        self.classes = self._get_classes()

        train_length = int(dataset_length * self.config.train_split / 100)
        validation_length = int(dataset_length * self.config.validation_split / 100)

        tf_dataset = tf.data.Dataset.from_tensor_slices(([point[0] for point in dataset], [point[1] for point in dataset]))

        self.train_data = self._build_pipeline(tf_dataset.take(train_length))
        self.validation_data = self._build_pipeline(tf_dataset.skip(train_length).take(validation_length))
        self.test_data = self._build_pipeline(tf_dataset.skip(train_length + validation_length))

    def _build_pipeline(self, dataset: tf.data.Dataset) -> tf.data.Dataset:
        return dataset\
                    .map(lambda file, label: tf.py_function(self._image_mapper, inp=(file, label), Tout=(tf.float32, tf.float32)), num_parallel_calls=tf.data.experimental.AUTOTUNE)\
                    .prefetch(tf.data.experimental.AUTOTUNE)\
                    .map(lambda images, labels: self._resize_image(images, labels))\
                    .batch(self.config.batch_size, drop_remainder=True)\
                    .prefetch(tf.data.experimental.AUTOTUNE)\
                    .cache()

    @abc.abstractmethod
    def _convert_to_image(self, file: str) -> np.array:
        pass

    def _image_mapper(self, file: tf.Tensor, label: tf.Tensor):
        if hasattr(self, 'CONVERT_OFFLINE'):
            conversion_function = self._load_converted_image
        else:
            conversion_function = self._convert_to_image

        image = tf.convert_to_tensor(conversion_function(file.numpy()), dtype=tf.float32) / 255.0
        label = tf.one_hot(label, self.num_classes)

        return image, label

    def _resize_image(self, images: tf.Tensor, labels: tf.Tensor) -> typing.Tuple[tf.Tensor, tf.Tensor]:
        images = tf.py_function(self.utils.resizer.resize, inp=(images,), Tout=tf.float32)
        images = tf.py_function(self.utils.sampler.resample, inp=(images, ), Tout=tf.float32)
        
        images.set_shape([self.utils.resizer.target_size[0], self.utils.resizer.target_size[1], self.utils.sampler.OUTPUT_CHANNELS])
        labels.set_shape([self.num_classes])

        return images, labels
