import os
import random

import argparse
import tensorflow as tf

import core.run

from core.config import DatasetConfig
from core.utils import DatasetUtils

def parse_args():
    parser = argparse.ArgumentParser('Malware Detection - Evaluation')

    parser.add_argument('--dataset-path', type=str, required=True, help='Path to the dataset')
    parser.add_argument('--model-path', type=str, required=True, help='Path to the trained model')
    parser.add_argument('--output-path', type=str, required=True, help='Path to the save the evaluation results')

    parser.add_argument('--batch-size', type=int, required=False, default=64, help='Batch size')
    parser.add_argument('--limit-dataset', type=int, required=False, default=0, help='Limit the dataset size')

    args = parser.parse_args()

    return args

def main():
    random.seed(1234)

    args = parse_args()

    if not len(args.checkpoint_path.split('_')) == 5:
        raise Exception('This checkpoint was not generated by this pipeline. Unable to load')

    (args.converter, args.model, args.resolution, args.resizer, args.sampler) = args.checkpoint_path.split('_')
    args.resolution = int(args.resolution.split('x')[0])

    (ConverterClass, ModelClass, ResizerClass, SamplerClass) = core.run.get_classes(args.converter, args.model, args.resizer, args.sampler, args.resolution)

    resizer = ResizerClass(target_size=ModelClass.INPUT_SIZE)
    sampler = SamplerClass()
    
    config = DatasetConfig(batch_size=args.batch_size, max_size=args.limit_dataset)
    utils = DatasetUtils(resizer, sampler)

    dataset = ConverterClass(args.dataset_path, config, utils)
    model = ModelClass(dataset.num_classes)

    latest_checkpoint = tf.train.latest_checkpoint(args.checkpoint_path)
    model.load_weights(latest_checkpoint)

    loss, acc = model.evaluate(dataset.validation_data, verbose=2)
    print("Restored model, accuracy: {:5.2f}%".format(100 * acc))
