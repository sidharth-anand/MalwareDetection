import os
import shutil
import random
import pickle
from datetime import datetime

import argparse
import sklearn.ensemble
import sklearn.metrics
import imblearn.over_sampling

import pandas as pd
import tensorflow as tf

import core.run

from core.config import DatasetConfig
from core.utils import DatasetUtils

def parse_args():
    converter_choices = core.run.get_import_choices('converters')
    models_choices = core.run.get_import_choices('models')
    resizer_choices = core.run.get_import_choices('resizers')
    sampler_choices = core.run.get_import_choices('samplers')

    parser = argparse.ArgumentParser('Malware Detection - Trainer')

    parser.add_argument('--dataset-path', type=str, required=True, help='Path to the dataset')
    parser.add_argument('--checkpoint-path', type=str, required=False, default='checkpoints', help='Path to the save the trained model checkpoints')
    parser.add_argument('--log-path', type=str, required=False, default='logs', help='Path to the save the training logs')

    parser.add_argument('--converter', type=str, required=True, choices=converter_choices, help='Converter to use')
    parser.add_argument('--resizer', type=str, required=False, default='simple', choices=resizer_choices, help='Resizer used on the dataset')
    parser.add_argument('--sampler', type=str, required=False, default='raw', choices=sampler_choices, help='Upsampler/Downsampler for the channels used on the dataset')

    parser.add_argument('--train-split', type=int, required=False, default=80, help='Train split')
    parser.add_argument('--validation-split', type=int, required=False, default=20, help='Validation split')
    parser.add_argument('--test-split', type=int, required=False, default=0, help='Test split')
    parser.add_argument('--resolution', type=int, required=False, default=224, help='Image resolution')
    parser.add_argument('--limit-dataset', type=int, required=False, default=0, help='Limit the dataset size')

    args = parser.parse_args()
    
    return args

def _convert_to_histogram(images: tf.Tensor) -> tf.Tensor:
    bins = 31
    window = (32, 256)

    patches = tf.image.extract_patches(images=images, sizes=(1, window[0], window[1], 1), strides=(1, 1, 1, 1), rates=(1, 1, 1, 1), padding='VALID')
    binned = tf.histogram_fixed_width_bins(patches, [0., 1.], nbins=bins)
    return tf.math.bincount(binned, minlength=bins+1)

def _get_model():
    trees = 51

    model = sklearn.ensemble.RandomForestClassifier(n_estimators=trees, random_state=0)
    return model

def main():
    random.seed(1234)

    args = parse_args()

    (ConverterClass, ModelClass, ResizerClass, SamplerClass) = core.run.get_classes(args.converter, 'allconv', args.resizer, args.sampler, args.resolution)

    resizer = ResizerClass(target_size=(256, 256))
    sampler = SamplerClass()
    
    config = DatasetConfig(args.train_split, args.validation_split, args.test_split, 1, args.limit_dataset)
    utils = DatasetUtils(resizer, sampler)

    dataset = ConverterClass(args.dataset_path, config, utils)
    model = _get_model()

    run_identifier = f'{args.converter}_ML_256x256x1_{args.resizer}_{args.sampler}'

    log_path = os.path.join(args.log_path, run_identifier)
    checkpoint_path = os.path.join(args.checkpoint_path, run_identifier, 'epoch-{epoch}.model')

    if os.path.exists(log_path):
        shutil.rmtree(log_path)

    data = []
    labels = []

    for (image, label) in dataset.train_data:
        data.append(_convert_to_histogram(image).numpy())
        labels.append(tf.math.argmax(label, axis=-1)[0].numpy())

    model.fit(data, labels)

    data = []
    labels = []

    for (image, label) in dataset.validation_data:
        data.append(_convert_to_histogram(image).numpy())
        labels.append(tf.math.argmax(label, axis=-1)[0].numpy())

    start = datetime.now()
    predictions = model.predict(data)
    end = datetime.now()
    print('Classification: ', (end - start).total_seconds(), len(data))
    accuracy = sklearn.metrics.accuracy_score(labels, predictions)
    print('Accuracy: ', accuracy)

    report = sklearn.metrics.classification_report(labels, predictions, target_names=list(dataset.classes.values())[:25], output_dict=True)
    report = pd.DataFrame(report).transpose()
    report.to_csv('report.csv')

    params = model.get_params()
    with open('params.dict', 'wb') as f:
        pickle.dump(model, f)

if __name__ == '__main__':
    main()
