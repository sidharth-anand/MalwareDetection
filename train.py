import glob
import importlib
import inspect
import typing
import os
import shutil

import argparse

import tensorflow as tf

from core.dataset import Dataset
from core.model import Model
from core.resize import Resize

from core.config import DatasetConfig
from core.utils import DatasetUtils

def get_choices_from_import(dir: str) -> typing.List[str]:
    choices = list(glob.glob(f'{dir}/*.py'))
    choices = [os.path.basename(choice).split('.')[0] for choice in choices]
    return choices

def parse_args():
    converter_choices = get_choices_from_import('converters')
    models_choices = get_choices_from_import('models')
    resizer_choices = get_choices_from_import('resize')

    parser = argparse.ArgumentParser('Malware Detection - Trainer')

    parser.add_argument('--dataset-path', type=str, required=True, help='Path to the dataset')
    parser.add_argument('--checkpoint-path', type=str, required=False, default='checkpoints', help='Path to the save the trained model checkpoints')
    parser.add_argument('--log-path', type=str, required=False, default='logs', help='Path to the save the training logs')
    
    parser.add_argument('--dataset-converter', type=str, required=True, choices=converter_choices, help='Converter used on the dataset')
    parser.add_argument('--dataset-model', type=str, required=True, choices=models_choices, help='Model used on the dataset')
    parser.add_argument('--dataset-resizer', type=str, required=False, default='simple', choices=resizer_choices, help='Resizer used on the dataset')

    parser.add_argument('--train-split', type=int, required=False, default=60, help='Train split')
    parser.add_argument('--validation-split', type=int, required=False, default=20, help='Validation split')
    parser.add_argument('--test-split', type=int, required=False, default=20, help='Test split')
    parser.add_argument('--batch-size', type=int, required=False, default=32, help='Batch size')
    parser.add_argument('--epochs', type=int, required=False, default=10, help='Number of Epochs')
    parser.add_argument('--limit-dataset', type=int, required=False, default=0, help='Limit the dataset size')

    args = parser.parse_args()
    return args

def get_imported_class(filename: str, base_class):
    module = importlib.import_module(filename)
    classes = inspect.getmembers(module, inspect.isclass)

    for _, cls in classes:
        if issubclass(cls, base_class) and cls != base_class:
            return cls

    raise Exception(f'Could not find a exported class in {filename} which is a subclass of {base_class}')

def main():
    args = parse_args()

    ConverterClass = get_imported_class(f'converters.{args.dataset_converter}', Dataset)
    ModelClass = get_imported_class(f'models.{args.dataset_model}', Model)
    ResizerClass = get_imported_class(f'resize.{args.dataset_resizer}', Resize)

    resizer = ResizerClass(target_size=ModelClass.INPUT_SIZE)
    
    config = DatasetConfig(args.train_split, args.validation_split, args.test_split, args.batch_size, args.limit_dataset)
    utils = DatasetUtils(resizer)

    dataset = ConverterClass(args.dataset_path, config, utils)
    model = ModelClass(dataset.num_classes)

    run_identifier = f'{args.dataset_converter}_{args.dataset_model}_{model.INPUT_SIZE[0]}x{model.INPUT_SIZE[1]}_{args.dataset_resizer}'

    log_path = os.path.join(args.log_path, run_identifier)
    checkpoint_path = os.path.join(args.checkpoint_path, run_identifier, 'epoch-{epoch}.model')

    if os.path.exists(log_path):
        shutil.rmtree(log_path)

    model.compile(optimizer=model.get_optimizer(), loss=model.get_loss(), metrics=model.get_metrics())
    model.fit(dataset.train_data, validation_data=dataset.validation_data, epochs=args.epochs, batch_size=1, callbacks=model.get_callbacks(log_path, checkpoint_path))

if __name__ == '__main__':
    main()
