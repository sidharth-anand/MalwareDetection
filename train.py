import os
import shutil
import random

import argparse

import core.run

from core.config import DatasetConfig
from core.utils import DatasetUtils

def parse_args():
    converter_choices = core.run.get_import_choices('converters')
    models_choices = core.run.get_import_choices('models')
    resizer_choices = core.run.get_import_choices('resizers')
    sampler_choices = core.run.get_import_choices('samplers')

    parser = argparse.ArgumentParser('Malware Detection - Trainer')

    parser.add_argument('--dataset-path', type=str, required=True, help='Path to the dataset')
    parser.add_argument('--checkpoint-path', type=str, required=False, default='checkpoints', help='Path to the save the trained model checkpoints')
    parser.add_argument('--log-path', type=str, required=False, default='logs', help='Path to the save the training logs')
    
    parser.add_argument('--converter', type=str, required=True, choices=converter_choices, help='Converter used on the dataset')
    parser.add_argument('--model', type=str, required=True, choices=models_choices, help='Model used on the dataset')
    parser.add_argument('--resizer', type=str, required=False, default='simple', choices=resizer_choices, help='Resizer used on the dataset')
    parser.add_argument('--sampler', type=str, required=False, default='raw', choices=sampler_choices, help='Upsampler/Downsampler for the channels used on the dataset')

    parser.add_argument('--train-split', type=int, required=False, default=80, help='Train split')
    parser.add_argument('--validation-split', type=int, required=False, default=20, help='Validation split')
    parser.add_argument('--test-split', type=int, required=False, default=0, help='Test split')
    parser.add_argument('--batch-size', type=int, required=False, default=32, help='Batch size')
    parser.add_argument('--epochs', type=int, required=False, default=10, help='Number of Epochs')
    parser.add_argument('--resolution', type=int, required=False, default=224, help='Image resolution')
    parser.add_argument('--limit-dataset', type=int, required=False, default=0, help='Limit the dataset size')

    args = parser.parse_args()
    
    return args

def main():
    random.seed(1234)

    args = parse_args()

    (ConverterClass, ModelClass, ResizerClass, SamplerClass) = core.run.get_classes(args.converter, args.model, args.resizer, args.sampler, args.resolution)

    resizer = ResizerClass(target_size=ModelClass.INPUT_SIZE)
    sampler = SamplerClass()
    
    config = DatasetConfig(args.train_split, args.validation_split, args.test_split, args.batch_size, args.limit_dataset)
    utils = DatasetUtils(resizer, sampler)

    dataset = ConverterClass(args.dataset_path, config, utils)
    model = ModelClass(dataset.num_classes)

    run_identifier = f'{args.converter}_{args.model}_{model.INPUT_SIZE[0]}x{model.INPUT_SIZE[1]}x{sampler.OUTPUT_CHANNELS}_{args.resizer}_{args.sampler}'

    log_path = os.path.join(args.log_path, run_identifier)
    checkpoint_path = os.path.join(args.checkpoint_path, run_identifier, 'epoch-{epoch}.model')

    if os.path.exists(log_path):
        shutil.rmtree(log_path)

    model.compile(optimizer=model.get_optimizer(), loss=model.get_loss(), metrics=model.get_metrics())
    model.fit(dataset.train_data, validation_data=dataset.validation_data, epochs=args.epochs, batch_size=1, callbacks=model.get_callbacks(log_path, checkpoint_path))

if __name__ == '__main__':
    main()
